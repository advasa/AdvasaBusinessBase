import json
import os
import logging
import boto3
import traceback
from datetime import datetime, timezone
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from common.slack_client import SlackClient
from common.monitoring_utils import lambda_handler_wrapper, performance_timer
import unicodedata
import hashlib
from sqlalchemy import create_engine, text
import gzip
import base64
from urllib.parse import quote_plus

# zengin-codeã®å‹•çš„ç®¡ç†
from package_manager import ZenginCodeManager


# Configure logging - will be replaced by monitoring wrapper
logger = logging.getLogger()
logger.setLevel(os.getenv('LOG_LEVEL', 'INFO'))

# SlackClientã®åˆæœŸã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆé€šçŸ¥ç”¨ï¼‰
try:
    # æ—¢å­˜ã®SlackClientã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒã‚ã‚Œã°åˆ©ç”¨
    slack_client_for_update = SlackClient() if os.getenv('SLACK_BOT_TOKEN') else None
except Exception as e:
    logger.warning(f"æ›´æ–°é€šçŸ¥ç”¨SlackClientåˆæœŸåŒ–ã‚¹ã‚­ãƒƒãƒ—: {str(e)}")
    slack_client_for_update = None

# zengin-codeã‚’å‹•çš„ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
zengin_manager = ZenginCodeManager(slack_client=slack_client_for_update)
success, error_message = zengin_manager.ensure_latest_version()

if success:
    from zengin_code import Bank
    logger.info("zengin-codeã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«æˆåŠŸã—ã¾ã—ãŸ")
else:
    # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚‹å ´åˆã¯è©³ç´°ã‚’è¨˜éŒ²
    if error_message:
        logger.error(f"zengin-codeç®¡ç†ã‚¨ãƒ©ãƒ¼: {error_message}")
    
    # æ—¢å­˜ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ç¶™ç¶šã‚’è©¦ã¿ã‚‹
    try:
        from zengin_code import Bank
        logger.warning("æ—¢å­˜ã®zengin-codeãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§å‡¦ç†ã‚’ç¶™ç¶šã—ã¾ã™")
    except ImportError as e:
        # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ã‚¨ãƒ©ãƒ¼: zengin-codeãŒå…¨ãåˆ©ç”¨ã§ããªã„
        error_msg = f"zengin-codeã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}"
        logger.critical(error_msg)
        
        # Slacké€šçŸ¥ã‚’è©¦ã¿ã‚‹
        if slack_client_for_update:
            try:
                slack_client_for_update.post_message(
                    text="ğŸš¨ ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ã‚¨ãƒ©ãƒ¼: zengin-codeãŒåˆ©ç”¨ã§ãã¾ã›ã‚“",
                    blocks=[
                        {
                            "type": "section",
                            "text": {
                                "type": "mrkdwn",
                                "text": f"Lambdaé–¢æ•°ã®å®Ÿè¡Œã‚’ç¶™ç¶šã§ãã¾ã›ã‚“:\n```{error_msg}```"
                            }
                        }
                    ]
                )
            except:
                pass
        
        raise ImportError(error_msg)

# AWS clients setup
dynamodb = boto3.resource('dynamodb')
secrets_manager = boto3.client('secretsmanager')
s3 = boto3.client('s3')

# Environment variables
DIFF_TABLE_NAME = os.getenv('DIFF_TABLE_NAME')
DATABASE_SECRET_ARN = os.getenv('DATABASE_SECRET_ARN')
ENVIRONMENT = os.getenv('ENVIRONMENT', 'dev')
S3_BUCKET_NAME = os.getenv('S3_BUCKET_NAME', f'{ENVIRONMENT}-zengin-diff-data')

@dataclass
class BankData:
    """éŠ€è¡Œãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«"""
    swift_code: str
    bank_name: str
    bank_name_kana: str
    branch_code: str
    branch_name: str
    branch_name_kana: str

@dataclass
class BankDiff:
    """å·®åˆ†ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«"""
    action: str  # "create", "update", "delete"
    key: str  # swift_code-branch_code
    old_data: Optional[BankData] = None
    new_data: Optional[BankData] = None
    total_accounts: int = 0
    active_users: int = 0

@dataclass
class BankUpdateRequestData:
    """æ›´æ–°ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿"""
    diffs: List[BankDiff]
    summary: str
    total_changes: int

class ZenginClient:
    """å…¨éŠ€å”ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self):
        self.bank_class = Bank
    
    def _normalize_bank_name(self, bank_name: str) -> str:
        """éŠ€è¡Œåã‚’æ­£è¦åŒ–"""
        if not bank_name:
            return bank_name
            
        # æ—¢ã«é©åˆ‡ãªæ¥å°¾è¾ãŒã‚ã‚‹å ´åˆã¯ãã®ã¾ã¾
        suffixes = ["éŠ€è¡Œ", "ä¿¡ç”¨é‡‘åº«", "ä¿¡ç”¨çµ„åˆ", "è¾²å”", "æ¼å”", "åŠ´åƒé‡‘åº«", "ä¿¡è¨—", "è¨¼åˆ¸"]
        for suffix in suffixes:
            if bank_name.endswith(suffix):
                return bank_name
                
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã€ŒéŠ€è¡Œã€ã‚’ä»˜ä¸
        return f"{bank_name}éŠ€è¡Œ"
    
    def _normalize_branch_name(self, branch_name: str) -> str:
        """æ”¯åº—åã‚’æ­£è¦åŒ–"""
        if not branch_name:
            return branch_name
            
        # æ—¢ã«é©åˆ‡ãªæ¥å°¾è¾ãŒã‚ã‚‹å ´åˆã¯ãã®ã¾ã¾
        suffixes = ["æ”¯åº—", "å–¶æ¥­éƒ¨", "å‡ºå¼µæ‰€", "ä»£ç†åº—", "æœ¬åº—", "åº—èˆ—", "ã‚»ãƒ³ã‚¿ãƒ¼", "ãƒ—ãƒ©ã‚¶"]
        for suffix in suffixes:
            if branch_name.endswith(suffix):
                return branch_name
                
        # ç‰¹æ®Šã‚±ãƒ¼ã‚¹
        if branch_name in ["æœ¬åº—", "å–¶æ¥­éƒ¨"]:
            return branch_name
            
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã€Œæ”¯åº—ã€ã‚’ä»˜ä¸
        return f"{branch_name}æ”¯åº—"

    def _convert_kana_to_hankaku(self, kana_text: str) -> str:
        """å…¨è§’ã‚«ã‚¿ã‚«ãƒŠã‚’åŠè§’ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›"""
        if not kana_text:
            return kana_text
            
        # NFKCæ­£è¦åŒ–
        normalized = unicodedata.normalize('NFKC', kana_text)
        
        # å…¨è§’ã‚«ã‚¿ã‚«ãƒŠ â†’ åŠè§’ã‚«ã‚¿ã‚«ãƒŠã®å¤‰æ›ãƒãƒƒãƒ—
        kana_map = {
            'ã‚¢': 'ï½±', 'ã‚¤': 'ï½²', 'ã‚¦': 'ï½³', 'ã‚¨': 'ï½´', 'ã‚ª': 'ï½µ',
            'ã‚«': 'ï½¶', 'ã‚­': 'ï½·', 'ã‚¯': 'ï½¸', 'ã‚±': 'ï½¹', 'ã‚³': 'ï½º',
            'ã‚µ': 'ï½»', 'ã‚·': 'ï½¼', 'ã‚¹': 'ï½½', 'ã‚»': 'ï½¾', 'ã‚½': 'ï½¿',
            'ã‚¿': 'ï¾€', 'ãƒ': 'ï¾', 'ãƒ„': 'ï¾‚', 'ãƒ†': 'ï¾ƒ', 'ãƒˆ': 'ï¾„',
            'ãƒŠ': 'ï¾…', 'ãƒ‹': 'ï¾†', 'ãƒŒ': 'ï¾‡', 'ãƒ': 'ï¾ˆ', 'ãƒ': 'ï¾‰',
            'ãƒ': 'ï¾Š', 'ãƒ’': 'ï¾‹', 'ãƒ•': 'ï¾Œ', 'ãƒ˜': 'ï¾', 'ãƒ›': 'ï¾',
            'ãƒ': 'ï¾', 'ãƒŸ': 'ï¾', 'ãƒ ': 'ï¾‘', 'ãƒ¡': 'ï¾’', 'ãƒ¢': 'ï¾“',
            'ãƒ¤': 'ï¾”', 'ãƒ¦': 'ï¾•', 'ãƒ¨': 'ï¾–',
            'ãƒ©': 'ï¾—', 'ãƒª': 'ï¾˜', 'ãƒ«': 'ï¾™', 'ãƒ¬': 'ï¾š', 'ãƒ­': 'ï¾›',
            'ãƒ¯': 'ï¾œ', 'ãƒ²': 'ï½¦', 'ãƒ³': 'ï¾',
            'ã‚¡': 'ï½§', 'ã‚£': 'ï½¨', 'ã‚¥': 'ï½©', 'ã‚§': 'ï½ª', 'ã‚©': 'ï½«',
            'ãƒƒ': 'ï½¯', 'ãƒ£': 'ï½¬', 'ãƒ¥': 'ï½­', 'ãƒ§': 'ï½®',
            'ã‚¬': 'ï½¶ï¾', 'ã‚®': 'ï½·ï¾', 'ã‚°': 'ï½¸ï¾', 'ã‚²': 'ï½¹ï¾', 'ã‚´': 'ï½ºï¾',
            'ã‚¶': 'ï½»ï¾', 'ã‚¸': 'ï½¼ï¾', 'ã‚º': 'ï½½ï¾', 'ã‚¼': 'ï½¾ï¾', 'ã‚¾': 'ï½¿ï¾',
            'ãƒ€': 'ï¾€ï¾', 'ãƒ‚': 'ï¾ï¾', 'ãƒ…': 'ï¾‚ï¾', 'ãƒ‡': 'ï¾ƒï¾', 'ãƒ‰': 'ï¾„ï¾',
            'ãƒ': 'ï¾Šï¾', 'ãƒ“': 'ï¾‹ï¾', 'ãƒ–': 'ï¾Œï¾', 'ãƒ™': 'ï¾ï¾', 'ãƒœ': 'ï¾ï¾',
            'ãƒ‘': 'ï¾Šï¾Ÿ', 'ãƒ”': 'ï¾‹ï¾Ÿ', 'ãƒ—': 'ï¾Œï¾Ÿ', 'ãƒš': 'ï¾ï¾Ÿ', 'ãƒ': 'ï¾ï¾Ÿ',
            'ãƒ´': 'ï½³ï¾',
            'ï¼': 'ï½°', 'ãƒ¼': 'ï½°'
        }
        
        result = ""
        for char in normalized:
            result += kana_map.get(char, char)
        
        return result

    def get_all_banks(self) -> List[BankData]:
        """zengin-codeã‹ã‚‰å…¨ã¦ã®éŠ€è¡Œãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆæ¨¡æ“¬å®Ÿè£…ï¼‰"""
        try:
            all_banks = self.bank_class.all
            
            bank_data_list = []
            for bank_code, bank_info in all_banks.items():
                # éŠ€è¡Œã®åŸºæœ¬æƒ…å ±ã‚’æ­£è¦åŒ–
                bank_name = self._normalize_bank_name(bank_info.name)
                bank_name_kana = self._convert_kana_to_hankaku(bank_info.kana)
                
                # å„æ”¯åº—ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                if hasattr(bank_info, 'branches') and bank_info.branches:
                    for branch_code, branch_info in bank_info.branches.items():
                        branch_name = self._normalize_branch_name(branch_info.name)
                        bank_data = BankData(
                            swift_code=bank_code,
                            bank_name=bank_name,
                            bank_name_kana=bank_name_kana,
                            branch_code=branch_code,
                            branch_name=branch_name,
                            branch_name_kana=self._convert_kana_to_hankaku(branch_info.kana)
                        )
                        bank_data_list.append(bank_data)
                else:
                    # æ”¯åº—æƒ…å ±ãŒãªã„å ´åˆã¯éŠ€è¡Œæœ¬ä½“ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿
                    bank_data = BankData(
                        swift_code=bank_code,
                        bank_name=bank_name,
                        bank_name_kana=bank_name_kana,
                        branch_code="001",  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æœ¬åº—ã‚³ãƒ¼ãƒ‰
                        branch_name="æœ¬åº—",
                        branch_name_kana=self._convert_kana_to_hankaku("ãƒ›ãƒ³ãƒ†ãƒ³")
                    )
                    bank_data_list.append(bank_data)
            
            logger.info(f"å–å¾—ã—ãŸéŠ€è¡Œãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(bank_data_list)}")
            return bank_data_list
            
        except Exception as e:
            logger.error(f"zengin_codeã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise

class DatabaseClient:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ - PostgreSQL via SQLAlchemy"""
    
    def __init__(self):
        self.db_credentials: Optional[Dict[str, Any]] = None
        self._engine = None
    
    def _get_db_credentials(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èªè¨¼æƒ…å ±ã‚’å–å¾—"""
        if self.db_credentials:
            return self.db_credentials
            
        try:
            response = secrets_manager.get_secret_value(SecretId=DATABASE_SECRET_ARN)
            self.db_credentials = json.loads(response['SecretString'])
            return self.db_credentials
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èªè¨¼æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise
    
    def _get_engine(self):
        """Create or return cached SQLAlchemy engine"""
        if self._engine:
            return self._engine
        creds = self._get_db_credentials()
        host = creds.get("host") or creds.get("endpoint")
        port = creds.get("port", 5432)
        dbname = creds.get("name") or creds.get("dbname") or creds.get("database")
        user = creds.get("username") or creds.get("user")
        password = creds.get("password") or creds.get("secret")
        
        # ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ­ã‚°ï¼ˆãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã®ä¸€éƒ¨ã‚’ãƒã‚¹ã‚¯ï¼‰
        logger.info(f"Database connection info - Host: {host}, Port: {port}, Database: {dbname}, User: {user}")
        logger.info(f"Password length: {len(password)}, First 3 chars: {password[:3] if password else 'None'}...")
        logger.info(f"Password repr: {repr(password[:10])}...")  # ç‰¹æ®Šæ–‡å­—ç¢ºèªç”¨
        logger.info(f"Password ends with: {repr(password[-3:])}")  # æœ«å°¾ã®æ–‡å­—ç¢ºèª
        
        # URLã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã‚’è¡Œã‚ãšã€connect_argsã§ç›´æ¥æ¸¡ã™
        db_url = f"postgresql+psycopg2://{host}:{port}/{dbname}"
        self._engine = create_engine(
            db_url,
            pool_pre_ping=True,
            connect_args={
                "user": user,
                "password": password,
                "sslmode": "require"
            }
        )
        return self._engine

    def get_mbank_data(self) -> List[Dict[str, Any]]:
        """ç¾åœ¨ã®MBankãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        try:
            engine = self._get_engine()
            with engine.connect() as conn:
                result = conn.execute(
                    text(
                        """
                        SELECT swift_code, bank_name, bank_name_kana, branch_code, branch_name, branch_name_kana
                        FROM m_bank
                        WHERE is_deleted = 0
                        """
                    )
                )
                data = [dict(row._mapping) for row in result]
                logger.info(f"MBankãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(data)}")
                return data
        except Exception as e:
            logger.error(f"MBankãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise
    
    def get_user_bank_account_impact_stats(self, swift_code: str, branch_code: str) -> Dict[str, int]:
        """æŒ‡å®šã•ã‚ŒãŸéŠ€è¡Œæ”¯åº—ã‚³ãƒ¼ãƒ‰ã«ç´ã¥ãUserBankAccountã®å½±éŸ¿çµ±è¨ˆã‚’å–å¾—"""
        engine = self._get_engine()
        try:
            with engine.connect() as conn:
                result = conn.execute(
                    text(
                        """
                        SELECT 
                            COUNT(uba.id)                    AS total_accounts,
                            COUNT(DISTINCT uba.user_id)      AS total_users,
                            COUNT(CASE WHEN u.use_status = 1 THEN 1 END)             AS active_user_accounts,
                            COUNT(DISTINCT CASE WHEN u.use_status = 1 THEN uba.user_id END) AS active_users
                        FROM user_bank_account uba
                        LEFT JOIN "user" u ON uba.user_id = u.id
                        WHERE uba.bank_swift_code = :swift_code
                          AND uba.branch_code = :branch_code
                          AND uba.is_deleted = 0
                        """
                    ),
                    {"swift_code": swift_code, "branch_code": branch_code},
                )
                row = result.fetchone()
                if not row:
                    return {"total_accounts": 0, "active_users": 0}
                return {
                    "total_accounts": row.total_accounts or 0,
                    "active_users": row.active_users or 0,
                }
        except Exception as e:
            logger.error(f"å½±éŸ¿çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {"total_accounts": 0, "active_users": 0}
    
    def get_user_bank_account_impact_stats_batch(self, bank_branch_pairs: List[tuple]) -> Dict[str, Dict[str, int]]:
        """è¤‡æ•°ã®éŠ€è¡Œæ”¯åº—ã‚³ãƒ¼ãƒ‰ã«ç´ã¥ãUserBankAccountã®å½±éŸ¿çµ±è¨ˆã‚’ä¸€æ‹¬å–å¾—"""
        if not bank_branch_pairs:
            return {}
        
        engine = self._get_engine()
        try:
            with engine.connect() as conn:
                # Create a temporary table for the batch query
                values_clause = ", ".join([f"('{swift}', '{branch}')" for swift, branch in bank_branch_pairs])
                
                result = conn.execute(
                    text(f"""
                        WITH bank_codes AS (
                            SELECT * FROM (VALUES {values_clause}) AS t(swift_code, branch_code)
                        )
                        SELECT 
                            uba.bank_swift_code,
                            uba.branch_code,
                            COUNT(uba.id) AS total_accounts,
                            COUNT(DISTINCT uba.user_id) AS total_users,
                            COUNT(CASE WHEN u.use_status = 1 THEN 1 END) AS active_user_accounts,
                            COUNT(DISTINCT CASE WHEN u.use_status = 1 THEN uba.user_id END) AS active_users
                        FROM bank_codes bc
                        LEFT JOIN user_bank_account uba 
                            ON uba.bank_swift_code = bc.swift_code 
                            AND uba.branch_code = bc.branch_code
                            AND uba.is_deleted = 0
                        LEFT JOIN "user" u ON uba.user_id = u.id
                        GROUP BY uba.bank_swift_code, uba.branch_code
                    """)
                )
                
                # Build result dictionary
                stats_dict = {}
                for row in result:
                    if row.bank_swift_code and row.branch_code:
                        key = f"{row.bank_swift_code}-{row.branch_code}"
                        stats_dict[key] = {
                            "total_accounts": row.total_accounts or 0,
                            "active_users": row.active_users or 0,
                        }
                
                # Fill in zeros for any missing entries
                for swift, branch in bank_branch_pairs:
                    key = f"{swift}-{branch}"
                    if key not in stats_dict:
                        stats_dict[key] = {"total_accounts": 0, "active_users": 0}
                
                return stats_dict
                
        except Exception as e:
            logger.error(f"ä¸€æ‹¬å½±éŸ¿çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            # Return zeros for all entries on error
            return {f"{swift}-{branch}": {"total_accounts": 0, "active_users": 0} 
                    for swift, branch in bank_branch_pairs}

class DiffDetector:
    """å·®åˆ†æ¤œå‡ºã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self):
        self.zengin_client = ZenginClient()
        self.db_client = DatabaseClient()
    
    def detect_differences(self) -> BankUpdateRequestData:
        """å·®åˆ†æ¤œå‡ºãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        logger.info("å·®åˆ†æ¤œå‡ºã‚’é–‹å§‹")
        
        try:
            # ç¾åœ¨ã®MBankãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            current_data = self.db_client.get_mbank_data()
            current_dict = {f"{item['swift_code']}-{item['branch_code']}": item 
                           for item in current_data}
            
            # zengin-codeã‹ã‚‰æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            latest_data = self.zengin_client.get_all_banks()
            latest_dict = {f"{item.swift_code}-{item.branch_code}": item 
                          for item in latest_data}
            
            diffs = []
            bank_codes_for_impact = []  # å½±éŸ¿çµ±è¨ˆãŒå¿…è¦ãªéŠ€è¡Œã‚³ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ
            
            # æ–°è¦è¿½åŠ ã¨æ›´æ–°ã‚’æ¤œå‡º
            for key, new_item in latest_dict.items():
                if key not in current_dict:
                    # æ–°è¦è¿½åŠ 
                    diff = BankDiff(
                        action="create",
                        key=key,
                        old_data=None,
                        new_data=new_item
                    )
                    diffs.append(diff)
                else:
                    # æ›´æ–°ãƒã‚§ãƒƒã‚¯
                    current_item = current_dict[key]
                    if self._is_data_different(current_item, new_item):
                        old_data = BankData(**current_item)
                        diff = BankDiff(
                            action="update",
                            key=key,
                            old_data=old_data,
                            new_data=new_item,
                            total_accounts=0,  # å¾Œã§ä¸€æ‹¬æ›´æ–°
                            active_users=0  # å¾Œã§ä¸€æ‹¬æ›´æ–°
                        )
                        diffs.append(diff)
                        bank_codes_for_impact.append((new_item.swift_code, new_item.branch_code))
            
            # å‰Šé™¤ã‚’æ¤œå‡º
            for key, current_item in current_dict.items():
                if key not in latest_dict:
                    old_data = BankData(**current_item)
                    diff = BankDiff(
                        action="delete",
                        key=key,
                        old_data=old_data,
                        new_data=None,
                        total_accounts=0,  # å¾Œã§ä¸€æ‹¬æ›´æ–°
                        active_users=0  # å¾Œã§ä¸€æ‹¬æ›´æ–°
                    )
                    diffs.append(diff)
                    bank_codes_for_impact.append((old_data.swift_code, old_data.branch_code))
            
            # å½±éŸ¿çµ±è¨ˆã‚’ä¸€æ‹¬å–å¾—
            if bank_codes_for_impact:
                logger.info(f"å½±éŸ¿çµ±è¨ˆã‚’ä¸€æ‹¬å–å¾—: {len(bank_codes_for_impact)}ä»¶")
                impact_stats = self.db_client.get_user_bank_account_impact_stats_batch(bank_codes_for_impact)
                
                # å·®åˆ†ã«å½±éŸ¿çµ±è¨ˆã‚’é©ç”¨
                for diff in diffs:
                    if diff.action in ["update", "delete"]:
                        stats = impact_stats.get(diff.key, {"total_accounts": 0, "active_users": 0})
                        diff.total_accounts = stats["total_accounts"]
                        diff.active_users = stats["active_users"]
            
            # ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ
            summary = self._create_summary(diffs)
            
            logger.info(f"å·®åˆ†æ¤œå‡ºå®Œäº†: {len(diffs)}ä»¶ã®å·®åˆ†ã‚’æ¤œå‡º")
            
            return BankUpdateRequestData(
                diffs=diffs,
                summary=summary,
                total_changes=len(diffs)
            )
            
        except Exception as e:
            logger.error(f"å·®åˆ†æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise
    
    def _is_data_different(self, current: Dict, new: BankData) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ãŒç•°ãªã‚‹ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        fields_to_check = [
            'bank_name', 'bank_name_kana', 'branch_name', 'branch_name_kana'
        ]
        suffix_sets: list[list[str]] = [["æ”¯åº—", "æ”¯æ‰€", "å‡ºå¼µæ‰€"]]
        flat_suffixes = set(sum(suffix_sets, []))

        def split_suffix(name: str):
            for s in flat_suffixes:
                if name.endswith(s):
                    return name[:-len(s)].strip(), s
            return name.strip(), ''

        def is_equivalent_suffix(s1: str, s2: str) -> bool:
            if s1 == s2:
                return True
            for group in suffix_sets:
                if s1 in group and s2 in group:
                    return True
            return False

        for field in fields_to_check:
            current_value = current.get(field)
            new_value = getattr(new, field)

            # None æ­£è¦åŒ–
            if current_value is None:
                current_value = ""
            if new_value is None:
                new_value = ""

            # ã‚«ãƒŠãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯åŠè§’ã‚«ãƒŠæ¯”è¼ƒ
            if field.endswith('_kana'):
                current_value = self.zengin_client._convert_kana_to_hankaku(str(current_value))
                new_value = self.zengin_client._convert_kana_to_hankaku(str(new_value))

            # ç‰¹æ®Š: branch_name ã® suffix åŒç¾©èªå‡¦ç†
            if field == 'branch_name':
                base_cur, suf_cur = split_suffix(str(current_value))
                base_new, suf_new = split_suffix(str(new_value))
                if base_cur != base_new:
                    return True
                if suf_cur == suf_new or is_equivalent_suffix(suf_cur, suf_new) or suf_cur == '' or suf_new == '':
                    continue
                return True
            else:
                if str(current_value).strip() != str(new_value).strip():
                    return True

        return False
    
    def _create_summary(self, diffs: List[BankDiff]) -> str:
        """å·®åˆ†ã®ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ"""
        create_count = len([d for d in diffs if d.action == "create"])
        update_count = len([d for d in diffs if d.action == "update"])
        delete_count = len([d for d in diffs if d.action == "delete"])
        
        # UserBankAccountå½±éŸ¿çµ±è¨ˆ
        total_affected_accounts = sum(
            diff.total_accounts for diff in diffs if diff.action in ["update", "delete"]
        )
        total_active_users = sum(
            diff.active_users for diff in diffs if diff.action in ["update", "delete"]
        )
        
        summary_parts = []
        if create_count > 0:
            summary_parts.append(f"æ–°è¦è¿½åŠ : {create_count}ä»¶")
        if update_count > 0:
            summary_parts.append(f"æ›´æ–°: {update_count}ä»¶")
        if delete_count > 0:
            summary_parts.append(f"å‰Šé™¤: {delete_count}ä»¶")
        
        if not summary_parts:
            return "å¤‰æ›´ãªã—"
        
        base_summary = "ã€".join(summary_parts)
        
        # å½±éŸ¿ã™ã‚‹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãŒã‚ã‚‹å ´åˆã¯è¿½è¨˜
        if total_affected_accounts > 0:
            base_summary += f" (UserBankAccountå½±éŸ¿: {total_affected_accounts}ä»¶ã€ç¨¼åƒãƒ¦ãƒ¼ã‚¶ãƒ¼: {total_active_users}å)"
        
        return base_summary


def store_diff_data_to_s3(diff_id: str, diffs: List[BankDiff]) -> str:
    """å¤§ããªå·®åˆ†ãƒ‡ãƒ¼ã‚¿ã‚’S3ã«ä¿å­˜"""
    try:
        # å·®åˆ†ãƒ‡ãƒ¼ã‚¿ã‚’JSONå½¢å¼ã«å¤‰æ›
        diffs_data = [asdict(diff) for diff in diffs]
        diffs_json = json.dumps(diffs_data, ensure_ascii=False)
        
        # gzipã§åœ§ç¸®
        compressed_data = gzip.compress(diffs_json.encode('utf-8'))
        
        # S3ã‚­ãƒ¼ã‚’ç”Ÿæˆ
        s3_key = f"diffs/{ENVIRONMENT}/{diff_id}/full_diffs.json.gz"
        
        # S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        s3.put_object(
            Bucket=S3_BUCKET_NAME,
            Key=s3_key,
            Body=compressed_data,
            ContentType='application/json',
            ContentEncoding='gzip',
            Metadata={
                'diff_id': diff_id,
                'original_size': str(len(diffs_json)),
                'compressed_size': str(len(compressed_data)),
                'diff_count': str(len(diffs))
            }
        )
        
        logger.info(f"å·®åˆ†ãƒ‡ãƒ¼ã‚¿ã‚’S3ã«ä¿å­˜: s3://{S3_BUCKET_NAME}/{s3_key}")
        return s3_key
        
    except Exception as e:
        logger.error(f"S3ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise

def check_recent_execution() -> Optional[Dict[str, Any]]:
    """éå»5åˆ†ä»¥å†…ã®å®Ÿè¡ŒãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    try:
        from datetime import datetime, timezone, timedelta
        
        # 5åˆ†å‰ã®æ™‚åˆ»ã‚’è¨ˆç®—
        five_minutes_ago = (datetime.now(timezone.utc) - timedelta(minutes=5)).isoformat()
        
        # DynamoDBã‹ã‚‰æœ€è¿‘ã®å®Ÿè¡Œã‚’ãƒã‚§ãƒƒã‚¯
        table = dynamodb.Table(DIFF_TABLE_NAME)
        response = table.scan(
            FilterExpression='#status = :status AND #timestamp > :recent_time',
            ExpressionAttributeNames={
                '#status': 'status',
                '#timestamp': 'timestamp'
            },
            ExpressionAttributeValues={
                ':status': 'pending',
                ':recent_time': five_minutes_ago
            },
            Limit=1  # 1ä»¶ã§ã‚‚è¦‹ã¤ã‹ã‚Œã°ååˆ†
        )
        
        if response.get('Items'):
            return response['Items'][0]
        return None
        
    except Exception as e:
        logger.error(f"é‡è¤‡å®Ÿè¡Œãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

def store_diff_data(update_request: BankUpdateRequestData, message_ts: str | None = None) -> str:
    """å·®åˆ†ãƒ‡ãƒ¼ã‚¿ã‚’DynamoDBã«ä¿å­˜"""
    try:
        table = dynamodb.Table(DIFF_TABLE_NAME)
        
        # ä¸€æ„ãªIDã‚’ç”Ÿæˆ
        timestamp = datetime.now(timezone.utc).isoformat()
        diff_id = f"diff-{datetime.now(timezone.utc).strftime('%Y%m%d-%H%M%S')}"
        
        # å…¨ã¦ã®å·®åˆ†ãƒ‡ãƒ¼ã‚¿ã‚’S3ã«ä¿å­˜
        diffs_data = [asdict(diff) for diff in update_request.diffs]
        diffs_json = json.dumps(diffs_data, ensure_ascii=False)
        data_size = len(diffs_json.encode('utf-8'))
        logger.info(f"å·®åˆ†ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {data_size / 1024:.2f}KB")
        
        # S3ã«å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆã‚µã‚¤ã‚ºã«é–¢ã‚ã‚‰ãšçµ±ä¸€å‡¦ç†ï¼‰
        s3_key = store_diff_data_to_s3(diff_id, update_request.diffs)
        
        # DynamoDBã«ã¯è¦ç´„æƒ…å ±ã®ã¿ã‚’ä¿å­˜ï¼ˆè¡¨ç¤ºç”¨ï¼‰
        summary_diffs = []
        for diff in update_request.diffs[:10]:  # æœ€åˆã®10ä»¶ã®ã¿è¡¨ç¤ºç”¨ã«ä¿å­˜
            summary_diff = {
                'action': diff.action,
                'key': diff.key,
                'swift_code': diff.new_data.swift_code if diff.new_data else diff.old_data.swift_code,
                'bank_name': diff.new_data.bank_name if diff.new_data else diff.old_data.bank_name,
                'branch_code': diff.new_data.branch_code if diff.new_data else diff.old_data.branch_code,
                'branch_name': diff.new_data.branch_name if diff.new_data else diff.old_data.branch_name,
            }
            summary_diffs.append(summary_diff)
        
        # DynamoDBã‚¢ã‚¤ãƒ†ãƒ ã‚’æ§‹ç¯‰ï¼ˆS3å‚ç…§ç‰ˆï¼‰
        item = {
            'id': diff_id,
            'timestamp': timestamp,
            'status': 'pending',
            'summary': update_request.summary,
            'total_changes': update_request.total_changes,
            'diffs': summary_diffs,  # è¡¨ç¤ºç”¨ã®è¦ç´„ãƒ‡ãƒ¼ã‚¿
            'diffs_s3_key': s3_key,  # S3ã®å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã¸ã®å‚ç…§
            'original_diff_count': len(update_request.diffs),
            'message_ts': message_ts,
            'environment': ENVIRONMENT,
            'ttl': int((datetime.now(timezone.utc).timestamp() + 30 * 24 * 60 * 60))  # 30æ—¥å¾Œã«TTL
        }
        
        # DynamoDBã«ä¿å­˜
        table.put_item(Item=item)
        
        logger.info(f"å·®åˆ†ãƒ‡ãƒ¼ã‚¿ã‚’DynamoDBã«ä¿å­˜: {diff_id}")
        return diff_id
        
    except Exception as e:
        logger.error(f"DynamoDBä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise

@lambda_handler_wrapper('zengin-diff-processor')
def handler(event: Dict[str, Any], context: Any, logger, metrics) -> Dict[str, Any]:
    """Lambdaé–¢æ•°ã®ãƒ¡ã‚¤ãƒ³ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    try:
        import uuid
        from datetime import datetime, timezone
        execution_id = str(uuid.uuid4())[:8]
        logger.info(f"å·®åˆ†å‡¦ç†ã‚’é–‹å§‹ [å®Ÿè¡ŒID: {execution_id}]", event_type="function_start", event_data=event, execution_id=execution_id)
        
        # zengin-codeã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
        try:
            import zengin_code
            zengin_version = getattr(zengin_code, '__version__', 'unknown')
            logger.info(f"zengin-codeãƒãƒ¼ã‚¸ãƒ§ãƒ³: {zengin_version}", execution_id=execution_id)
            metrics.emit_business_metric('ZenginCodeVersion', {'version': zengin_version})
        except Exception as e:
            logger.warning(f"zengin-codeãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}", execution_id=execution_id)
        
        # å®Ÿè¡Œãƒ­ãƒƒã‚¯ã‚’ç¢ºèªãƒ»è¨­å®šï¼ˆé‡è¤‡å®Ÿè¡Œé˜²æ­¢ï¼‰
        lock_key = f"diff-processor-lock-{datetime.now(timezone.utc).strftime('%Y-%m-%d-%H')}"
        try:
            # é‡è¤‡å®Ÿè¡Œãƒã‚§ãƒƒã‚¯ï¼ˆéå»5åˆ†ä»¥å†…ã®å®Ÿè¡ŒãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼‰
            recent_execution = check_recent_execution()
            if recent_execution:
                logger.warning(f"æœ€è¿‘ã®å®Ÿè¡Œã‚’æ¤œå‡º [å®Ÿè¡ŒID: {execution_id}] - ã‚¹ã‚­ãƒƒãƒ—", 
                             recent_execution=recent_execution, execution_id=execution_id)
                return {
                    'statusCode': 200,
                    'body': json.dumps({
                        'message': 'æœ€è¿‘ã®å®Ÿè¡ŒãŒæ¤œå‡ºã•ã‚ŒãŸãŸã‚ã‚¹ã‚­ãƒƒãƒ—',
                        'recent_execution': recent_execution
                    }, ensure_ascii=False)
                }
        except Exception as e:
            logger.warning(f"é‡è¤‡å®Ÿè¡Œãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ [å®Ÿè¡ŒID: {execution_id}]: {str(e)}", execution_id=execution_id)
        
        # å·®åˆ†æ¤œå‡ºã®å®Ÿè¡Œ
        with performance_timer(logger, metrics, 'diff_detection'):
            diff_detector = DiffDetector()
            update_request = diff_detector.detect_differences()
        
        if update_request.total_changes == 0:
            logger.info(f"å¤‰æ›´ãªã— [å®Ÿè¡ŒID: {execution_id}]", total_changes=0, execution_id=execution_id)
            metrics.emit_business_metric('NoChangesDetected')
            
            # Slacké€šçŸ¥ã‚’é€ä¿¡ (å¤‰æ›´ãªã—)
            slack_client = SlackClient()
            slack_client.send_no_changes_notification()
            logger.info(f"å¤‰æ›´ãªã—é€šçŸ¥é€ä¿¡å®Œäº† [å®Ÿè¡ŒID: {execution_id}]", execution_id=execution_id)
            
            return {
                'statusCode': 200,
                'body': json.dumps({
                    'message': 'å¤‰æ›´ãªã—',
                    'total_changes': 0
                }, ensure_ascii=False)
            }
        
        # Slacké€šçŸ¥ã‚’é€ä¿¡ via Bot Token
        with performance_timer(logger, metrics, 'slack_notification'):
            slack_client = SlackClient()
            logger.info(f"Slacké€šçŸ¥é€ä¿¡é–‹å§‹ [å®Ÿè¡ŒID: {execution_id}] - å¤‰æ›´æ•°: {update_request.total_changes}", execution_id=execution_id)
            notification_result = slack_client.send_diff_notification(update_request)
            message_ts = notification_result.get('ts') if isinstance(notification_result, dict) else None
            logger.info(f"Slacké€šçŸ¥é€ä¿¡å®Œäº† [å®Ÿè¡ŒID: {execution_id}] - message_ts: {message_ts}", execution_id=execution_id)
        
        # å·®åˆ†ãƒ‡ãƒ¼ã‚¿ã‚’DynamoDBã«ä¿å­˜
        with performance_timer(logger, metrics, 'dynamodb_save'):
            diff_id = store_diff_data(update_request, message_ts=message_ts)
        
        # CSV ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦Slackã«é€ä¿¡
        csv_upload_result = None
        if message_ts and notification_result.get('ok'):
            with performance_timer(logger, metrics, 'csv_upload'):
                try:
                    csv_file_id = slack_client.send_csv_diff(update_request, message_ts)
                    csv_upload_result = {'file_id': csv_file_id, 'status': 'success'}
                    logger.info(f"CSV uploaded to Slack thread: {csv_file_id}")
                except Exception as e:
                    logger.error(f"CSV upload error: {str(e)}")
                    csv_upload_result = {'status': 'error', 'error': str(e)}
        
        # ãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’é€ä¿¡
        metrics.emit_business_metric('DiffProcessingCompleted')
        metrics.emit_count_metric('ChangesDetected', update_request.total_changes)
        
        logger.info(f"å·®åˆ†å‡¦ç†å®Œäº† [å®Ÿè¡ŒID: {execution_id}]", 
                   total_changes=update_request.total_changes,
                   diff_id=diff_id,
                   slack_message_ts=message_ts,
                   execution_id=execution_id)
        
        return {
            'statusCode': 200,
            'body': json.dumps({
                'message': 'å·®åˆ†æ¤œå‡ºãƒ»é€šçŸ¥å®Œäº†',
                'diff_id': diff_id,
                'total_changes': update_request.total_changes,
                'summary': update_request.summary,
                'notification_result': notification_result,
                'csv_upload_result': csv_upload_result
            }, ensure_ascii=False)
        }
        
    except Exception as e:
        error_type = type(e).__name__
        error_message = f"å·®åˆ†å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}"
        
        logger.error("å·®åˆ†å‡¦ç†ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ", 
                    error=error_message,
                    error_type=error_type,
                    traceback=traceback.format_exc())
        
        metrics.emit_error_metric(error_type, {'Operation': 'diff_processing'})
        
        return {
            'statusCode': 500,
            'body': json.dumps({
                'error': error_message,
                'error_type': error_type
            }, ensure_ascii=False)
        }

if __name__ == "__main__":
    # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆç”¨
    test_event = {"trigger": "manual"}
    test_context = {}
    result = handler(test_event, test_context)
    print(json.dumps(result, indent=2, ensure_ascii=False))